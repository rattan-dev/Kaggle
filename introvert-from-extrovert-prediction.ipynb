{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 91718,
          "databundleVersionId": 12738969,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Introvert vs Extrovert Personality Prediction\n\n**A comprehensive machine learning analysis to predict personality types based on behavioral features**\n\nThis notebook implements a robust machine learning pipeline for predicting whether an individual is an introvert or extrovert based on various behavioral and social characteristics. The analysis follows best practices for data preprocessing, feature engineering, model training, and evaluation.\n\n---",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 1. Introduction\n\nPersonality prediction is a fascinating area where machine learning can provide insights into human behavior patterns. In this analysis, we'll build a classification model to predict introvert vs extrovert personality types using features such as:\n\n- Time spent alone\n- Stage fear presence\n- Social event attendance frequency\n- Outdoor activity preferences\n- Social interaction effects\n- Friend circle size\n- Social media posting frequency\n\nOur approach emphasizes reproducibility, proper preprocessing, and comprehensive evaluation to ensure robust and reliable predictions.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 2. Data Loading and Setup\n\nWe'll start by importing necessary libraries and setting up our environment for reproducible results.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Set random seeds for reproducibility\nimport random\nimport numpy as np\nfrom sklearn.utils import check_random_state\n\n# Set global random seeds\nRANDOM_STATE = 42\nrandom.seed(RANDOM_STATE)\nnp.random.seed(RANDOM_STATE)\n\n# Core libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Sklearn imports\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, RandomizedSearchCV\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# Set plotting style\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\nprint(\"Environment setup complete!\")\nprint(f\"Random state set to: {RANDOM_STATE}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Check available data files\nimport os\n\ndata_path = '../input/playground-series-s5e7/'\nif os.path.exists(data_path):\n    print(\"Available data files:\")\n    for filename in os.listdir(data_path):\n        print(f\"  - {filename}\")\nelse:\n    print(\"Data directory not found. Using alternative path.\")\n    # Alternative for local development\n    data_path = './'",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Load datasets\ntry:\n    # Load training data\n    train_data = pd.read_csv(os.path.join(data_path, 'train.csv'))\n    test_data = pd.read_csv(os.path.join(data_path, 'test.csv'))\n    \n    print(\"Data loaded successfully!\")\n    print(f\"Training data shape: {train_data.shape}\")\n    print(f\"Test data shape: {test_data.shape}\")\nexcept FileNotFoundError:\n    print(\"Data files not found. Please ensure the data files are in the correct directory.\")\nexcept Exception as e:\n    print(f\"Error loading data: {e}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 3. Exploratory Data Analysis (EDA)\n\nLet's examine our data to understand its structure, identify patterns, and detect any data quality issues.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Display basic information about the training data\nprint(\"=== TRAINING DATA OVERVIEW ===\")\nprint(f\"Shape: {train_data.shape}\")\nprint(f\"\\nColumn names: {list(train_data.columns)}\")\nprint(f\"\\nData types:\")\nprint(train_data.dtypes)\nprint(f\"\\nFirst few rows:\")\ntrain_data.head()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Check for missing values\nprint(\"=== MISSING VALUES ANALYSIS ===\")\nmissing_train = train_data.isnull().sum()\nmissing_test = test_data.isnull().sum()\n\nprint(\"Training data missing values:\")\nfor col, count in missing_train.items():\n    if count > 0:\n        print(f\"  {col}: {count} ({count/len(train_data)*100:.2f}%)\")\n        \nprint(\"\\nTest data missing values:\")\nfor col, count in missing_test.items():\n    if count > 0:\n        print(f\"  {col}: {count} ({count/len(test_data)*100:.2f}%)\")\n        \nif missing_train.sum() == 0 and missing_test.sum() == 0:\n    print(\"No missing values found in either dataset.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Analyze target variable distribution\nprint(\"=== TARGET VARIABLE ANALYSIS ===\")\ntarget_counts = train_data['Personality'].value_counts()\nprint(f\"Target distribution:\")\nfor personality, count in target_counts.items():\n    print(f\"  {personality}: {count} ({count/len(train_data)*100:.2f}%)\")\n\n# Visualize target distribution\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n# Bar plot\ntarget_counts.plot(kind='bar', ax=ax1, color=['skyblue', 'lightcoral'])\nax1.set_title('Personality Type Distribution')\nax1.set_xlabel('Personality Type')\nax1.set_ylabel('Count')\nax1.tick_params(axis='x', rotation=0)\n\n# Pie chart\nax2.pie(target_counts.values, labels=target_counts.index, autopct='%1.1f%%', \n        colors=['skyblue', 'lightcoral'], startangle=90)\nax2.set_title('Personality Type Proportions')\n\nplt.tight_layout()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Analyze numerical features\nprint(\"=== NUMERICAL FEATURES ANALYSIS ===\")\nnumerical_cols = ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside', \n                  'Friends_circle_size', 'Post_frequency']\n\nprint(\"Descriptive statistics for numerical features:\")\nprint(train_data[numerical_cols].describe())\n\n# Visualize numerical features\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.ravel()\n\nfor i, col in enumerate(numerical_cols):\n    train_data[col].hist(bins=20, ax=axes[i], alpha=0.7, color='skyblue')\n    axes[i].set_title(f'Distribution of {col}')\n    axes[i].set_xlabel(col)\n    axes[i].set_ylabel('Frequency')\n\n# Remove extra subplot\naxes[-1].remove()\n\nplt.tight_layout()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Analyze categorical features\nprint(\"=== CATEGORICAL FEATURES ANALYSIS ===\")\ncategorical_cols = ['Stage_fear', 'Drained_after_socializing']\n\nfor col in categorical_cols:\n    print(f\"\\n{col} distribution:\")\n    value_counts = train_data[col].value_counts()\n    for value, count in value_counts.items():\n        print(f\"  {value}: {count} ({count/len(train_data)*100:.2f}%)\")\n\n# Visualize categorical features\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\nfor i, col in enumerate(categorical_cols):\n    value_counts = train_data[col].value_counts()\n    value_counts.plot(kind='bar', ax=axes[i], color=['lightgreen', 'salmon'])\n    axes[i].set_title(f'{col} Distribution')\n    axes[i].set_xlabel(col)\n    axes[i].set_ylabel('Count')\n    axes[i].tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 4. Data Preprocessing\n\nWe'll implement a robust preprocessing pipeline that handles missing values and categorical variables using sklearn's best practices.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Prepare data for preprocessing\n# Extract features and target\ny = train_data['Personality']\nX = train_data.drop(['id', 'Personality'], axis=1)\nX_test_final = test_data.drop(['id'], axis=1)\n\nprint(\"=== DATA PREPARATION ===\")\nprint(f\"Features shape: {X.shape}\")\nprint(f\"Target shape: {y.shape}\")\nprint(f\"Test data shape: {X_test_final.shape}\")\nprint(f\"\\nFeature columns: {list(X.columns)}\")\n\n# Identify column types for preprocessing\nnumerical_features = ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside', \n                     'Friends_circle_size', 'Post_frequency']\ncategorical_features = ['Stage_fear', 'Drained_after_socializing']\n\nprint(f\"\\nNumerical features: {numerical_features}\")\nprint(f\"Categorical features: {categorical_features}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Create preprocessing pipeline\n# For numerical features: impute with median (robust to outliers)\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median'))\n])\n\n# For categorical features: impute with most frequent value, then one-hot encode\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_features),\n        ('cat', categorical_transformer, categorical_features)\n    ]\n)\n\nprint(\"=== PREPROCESSING PIPELINE CREATED ===\")\nprint(\"Numerical features: Median imputation\")\nprint(\"Categorical features: Most frequent imputation + One-hot encoding\")\nprint(f\"\\nPipeline steps:\")\nprint(f\"1. Numerical transformer: {numerical_features}\")\nprint(f\"2. Categorical transformer: {categorical_features}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Split the data into training and validation sets\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n)\n\nprint(\"=== DATA SPLITTING ===\")\nprint(f\"Training set size: {X_train.shape[0]} samples\")\nprint(f\"Validation set size: {X_valid.shape[0]} samples\")\nprint(f\"Test set size: {X_test_final.shape[0]} samples\")\n\n# Check target distribution in splits\nprint(f\"\\nTraining set target distribution:\")\ntrain_dist = y_train.value_counts(normalize=True)\nfor personality, prop in train_dist.items():\n    print(f\"  {personality}: {prop:.3f}\")\n    \nprint(f\"\\nValidation set target distribution:\")\nvalid_dist = y_valid.value_counts(normalize=True)\nfor personality, prop in valid_dist.items():\n    print(f\"  {personality}: {prop:.3f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 5. Feature Engineering\n\nWe'll create new features that might help improve model performance by capturing additional patterns in the data.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def create_engineered_features(df):\n    \"\"\"\n    Create engineered features from existing data\n    \"\"\"\n    df_eng = df.copy()\n    \n    # Feature 1: Social engagement ratio (social activities vs alone time)\n    # Higher values indicate more social engagement\n    df_eng['Social_Engagement_Ratio'] = (\n        df_eng['Social_event_attendance'] + df_eng['Going_outside'] + df_eng['Post_frequency']\n    ) / (df_eng['Time_spent_Alone'] + 1)  # +1 to avoid division by zero\n    \n    # Feature 2: Social network size relative to activity\n    # This captures how active someone is relative to their friend circle\n    df_eng['Activity_Per_Friend'] = (\n        df_eng['Social_event_attendance'] + df_eng['Post_frequency']\n    ) / (df_eng['Friends_circle_size'] + 1)  # +1 to avoid division by zero\n    \n    # Feature 3: Introversion indicator score\n    # Combine features that typically indicate introversion\n    df_eng['Potential_Introversion_Score'] = (\n        df_eng['Time_spent_Alone'] * 0.4 +\n        (10 - df_eng['Social_event_attendance'].fillna(df_eng['Social_event_attendance'].median())) * 0.3 +  # Invert social attendance\n        (10 - df_eng['Going_outside'].fillna(df_eng['Going_outside'].median())) * 0.3  # Invert going outside\n    )\n    \n    return df_eng\n\n# Apply feature engineering to our datasets\nprint(\"=== FEATURE ENGINEERING ===\")\nX_train_eng = create_engineered_features(X_train)\nX_valid_eng = create_engineered_features(X_valid)\nX_test_eng = create_engineered_features(X_test_final)\n\nprint(f\"Original features: {X_train.shape[1]}\")\nprint(f\"Features after engineering: {X_train_eng.shape[1]}\")\nprint(f\"New features added: {X_train_eng.shape[1] - X_train.shape[1]}\")\n\nprint(f\"\\nNew feature names:\")\nnew_features = [col for col in X_train_eng.columns if col not in X_train.columns]\nfor feature in new_features:\n    print(f\"  - {feature}\")\n\n# Update feature lists for preprocessing\nnumerical_features_eng = numerical_features + new_features\nprint(f\"\\nUpdated numerical features: {numerical_features_eng}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Update preprocessing pipeline with engineered features\npreprocessor_eng = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_features_eng),\n        ('cat', categorical_transformer, categorical_features)\n    ]\n)\n\n# Display some statistics about the new features\nprint(\"=== NEW FEATURE STATISTICS ===\")\nfor feature in new_features:\n    print(f\"\\n{feature}:\")\n    print(f\"  Mean: {X_train_eng[feature].mean():.3f}\")\n    print(f\"  Std: {X_train_eng[feature].std():.3f}\")\n    print(f\"  Min: {X_train_eng[feature].min():.3f}\")\n    print(f\"  Max: {X_train_eng[feature].max():.3f}\")\n\n# Visualize correlation between new features and target\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\nfor i, feature in enumerate(new_features):\n    for personality in ['Introvert', 'Extrovert']:\n        data = X_train_eng[y_train == personality][feature]\n        axes[i].hist(data, alpha=0.6, label=personality, bins=20)\n    \n    axes[i].set_title(f'{feature} by Personality')\n    axes[i].set_xlabel(feature)\n    axes[i].set_ylabel('Frequency')\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 6. Model Training and Hyperparameter Tuning\n\nWe'll use a RandomForestClassifier with hyperparameter tuning to find the optimal model configuration.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Create the complete pipeline with preprocessing and model\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor_eng),\n    ('classifier', RandomForestClassifier(random_state=RANDOM_STATE))\n])\n\nprint(\"=== BASELINE MODEL TRAINING ===\")\n# Train baseline model\npipeline.fit(X_train_eng, y_train)\n\n# Make predictions on validation set\ny_pred_baseline = pipeline.predict(X_valid_eng)\ny_pred_proba_baseline = pipeline.predict_proba(X_valid_eng)\n\n# Evaluate baseline performance\nbaseline_accuracy = accuracy_score(y_valid, y_pred_baseline)\nprint(f\"Baseline model accuracy: {baseline_accuracy:.4f}\")\n\n# Cross-validation score\ncv_scores = cross_val_score(pipeline, X_train_eng, y_train, cv=5, scoring='accuracy')\nprint(f\"Cross-validation accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Hyperparameter tuning with RandomizedSearchCV\nprint(\"=== HYPERPARAMETER TUNING ===\")\nprint(\"Searching for optimal hyperparameters...\")\n\n# Define parameter grid\nparam_grid = {\n    'classifier__n_estimators': [100, 200, 300],\n    'classifier__max_depth': [10, 20, None],\n    'classifier__min_samples_split': [2, 5, 10],\n    'classifier__min_samples_leaf': [1, 2, 4],\n    'classifier__max_features': ['sqrt', 'log2']\n}\n\n# Create randomized search\nrandom_search = RandomizedSearchCV(\n    pipeline,\n    param_distributions=param_grid,\n    n_iter=20,  # Number of parameter settings sampled\n    cv=5,\n    scoring='accuracy',\n    random_state=RANDOM_STATE,\n    n_jobs=-1,  # Use all available cores\n    verbose=1\n)\n\n# Fit the randomized search\nrandom_search.fit(X_train_eng, y_train)\n\nprint(f\"\\nBest parameters found:\")\nfor param, value in random_search.best_params_.items():\n    print(f\"  {param}: {value}\")\n\nprint(f\"\\nBest cross-validation score: {random_search.best_score_:.4f}\")\n\n# Get the best model\nbest_model = random_search.best_estimator_",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 7. Model Evaluation\n\nNow let's thoroughly evaluate our optimized model using multiple metrics and visualizations.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Make predictions with the best model\ny_pred_optimized = best_model.predict(X_valid_eng)\ny_pred_proba_optimized = best_model.predict_proba(X_valid_eng)\n\nprint(\"=== MODEL EVALUATION ===\")\nprint(f\"Optimized model accuracy: {accuracy_score(y_valid, y_pred_optimized):.4f}\")\nprint(f\"Improvement over baseline: {accuracy_score(y_valid, y_pred_optimized) - baseline_accuracy:.4f}\")\n\n# Detailed classification report\nprint(f\"\\nClassification Report:\")\nprint(classification_report(y_valid, y_pred_optimized))\n\n# Confusion matrix\nconf_matrix = confusion_matrix(y_valid, y_pred_optimized)\nprint(f\"\\nConfusion Matrix:\")\nprint(conf_matrix)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Visualize confusion matrix and ROC curve\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Confusion matrix heatmap\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Extrovert', 'Introvert'], \n            yticklabels=['Extrovert', 'Introvert'], \n            ax=axes[0])\naxes[0].set_title('Confusion Matrix')\naxes[0].set_xlabel('Predicted')\naxes[0].set_ylabel('Actual')\n\n# ROC Curve\n# Convert target to binary for ROC curve\ny_valid_binary = (y_valid == 'Extrovert').astype(int)\ny_prob_extrovert = y_pred_proba_optimized[:, 1]  # Probability of being extrovert\n\nfpr, tpr, _ = roc_curve(y_valid_binary, y_prob_extrovert)\nroc_auc = auc(fpr, tpr)\n\naxes[1].plot(fpr, tpr, color='darkorange', lw=2, \n             label=f'ROC curve (AUC = {roc_auc:.3f})')\naxes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[1].set_xlim([0.0, 1.0])\naxes[1].set_ylim([0.0, 1.05])\naxes[1].set_xlabel('False Positive Rate')\naxes[1].set_ylabel('True Positive Rate')\naxes[1].set_title('ROC Curve')\naxes[1].legend(loc=\"lower right\")\n\nplt.tight_layout()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 8. Feature Importance Analysis\n\nLet's analyze which features are most important for predicting personality types.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Get feature importances from the trained model\nfeature_importances = best_model.named_steps['classifier'].feature_importances_\n\n# Get feature names after preprocessing\n# This is a bit complex due to the ColumnTransformer\nfeature_names = []\n\n# Add numerical feature names\nfeature_names.extend(numerical_features_eng)\n\n# Add categorical feature names (after one-hot encoding)\ncat_encoder = best_model.named_steps['preprocessor'].named_transformers_['cat']\ncat_feature_names = cat_encoder.named_steps['onehot'].get_feature_names_out(categorical_features)\nfeature_names.extend(cat_feature_names)\n\n# Create feature importance dataframe\nimportance_df = pd.DataFrame({\n    'Feature': feature_names,\n    'Importance': feature_importances\n}).sort_values('Importance', ascending=False)\n\nprint(\"=== FEATURE IMPORTANCE ANALYSIS ===\")\nprint(\"Top 10 most important features:\")\nfor i, (_, row) in enumerate(importance_df.head(10).iterrows()):\n    print(f\"{i+1:2d}. {row['Feature']:<30} {row['Importance']:.4f}\")\n\n# Visualize feature importances\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n\n# Bar plot of top 10 features\ntop_features = importance_df.head(10)\ncolors = plt.cm.viridis(np.linspace(0, 1, len(top_features)))\nax1.barh(range(len(top_features)), top_features['Importance'], color=colors)\nax1.set_yticks(range(len(top_features)))\nax1.set_yticklabels(top_features['Feature'])\nax1.set_xlabel('Feature Importance')\nax1.set_title('Top 10 Feature Importances')\nax1.invert_yaxis()\n\n# Pie chart of feature importance categories\noriginal_features = importance_df[importance_df['Feature'].isin(numerical_features)]['Importance'].sum()\nengineered_features = importance_df[importance_df['Feature'].isin(new_features)]['Importance'].sum()\ncategorical_encoded = importance_df[~importance_df['Feature'].isin(numerical_features_eng)]['Importance'].sum()\n\nlabels = ['Original Numerical', 'Engineered Features', 'Categorical (Encoded)']\nsizes = [original_features, engineered_features, categorical_encoded]\ncolors = ['skyblue', 'lightgreen', 'lightcoral']\n\nax2.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors, startangle=90)\nax2.set_title('Feature Importance by Category')\n\nplt.tight_layout()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Analyze feature importance insights\nprint(\"\\n=== FEATURE IMPORTANCE INSIGHTS ===\")\n\n# Most important original features\noriginal_importance = importance_df[importance_df['Feature'].isin(numerical_features)]\nif len(original_importance) > 0:\n    print(f\"Most important original feature: {original_importance.iloc[0]['Feature']} ({original_importance.iloc[0]['Importance']:.4f})\")\n\n# Most important engineered features\nengineered_importance = importance_df[importance_df['Feature'].isin(new_features)]\nif len(engineered_importance) > 0:\n    print(f\"Most important engineered feature: {engineered_importance.iloc[0]['Feature']} ({engineered_importance.iloc[0]['Importance']:.4f})\")\n\n# Categorical feature importance\ncat_importance = importance_df[~importance_df['Feature'].isin(numerical_features_eng)]\nif len(cat_importance) > 0:\n    print(f\"Most important categorical feature: {cat_importance.iloc[0]['Feature']} ({cat_importance.iloc[0]['Importance']:.4f})\")\n\nprint(f\"\\nFeature engineering contribution: {engineered_features:.3f} ({engineered_features/importance_df['Importance'].sum()*100:.1f}% of total importance)\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 9. Final Predictions\n\nNow let's make predictions on the test set using our optimized model.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Make predictions on test set\nprint(\"=== FINAL PREDICTIONS ===\")\ntest_predictions = best_model.predict(X_test_eng)\ntest_predictions_proba = best_model.predict_proba(X_test_eng)\n\nprint(f\"Test predictions made for {len(test_predictions)} samples\")\nprint(f\"Prediction distribution:\")\nunique, counts = np.unique(test_predictions, return_counts=True)\nfor pred, count in zip(unique, counts):\n    print(f\"  {pred}: {count} ({count/len(test_predictions)*100:.1f}%)\")\n\n# Create submission dataframe\nsubmission = pd.DataFrame({\n    'id': test_data['id'],\n    'Personality': test_predictions\n})\n\nprint(f\"\\nSubmission dataframe created with shape: {submission.shape}\")\nprint(\"First few predictions:\")\nprint(submission.head())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Save submission file\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file saved as 'submission.csv'\")\n\n# Display final submission sample\nprint(\"\\nFinal submission sample:\")\nprint(submission.tail(10))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 10. Conclusions\n\n### Key Findings\n\nOur comprehensive analysis of personality prediction has yielded several important insights:\n\n**Model Performance:**\n- Our optimized RandomForestClassifier achieved strong performance on the validation set\n- Hyperparameter tuning provided meaningful improvements over the baseline model\n- Cross-validation confirmed the model's robustness and generalizability\n\n**Feature Insights:**\n- **Most Important Features:** The analysis revealed which behavioral patterns are most predictive of personality type\n- **Feature Engineering Impact:** Our engineered features contributed meaningfully to model performance, suggesting that derived metrics can capture important personality indicators\n- **Categorical vs Numerical:** Both types of features proved valuable, with proper encoding being crucial for categorical variables\n\n**Data Quality:**\n- Missing value handling using appropriate imputation strategies (median for numerical, most frequent for categorical) ensured robust preprocessing\n- Stratified splitting maintained balanced class distributions across train/validation sets\n\n### Technical Improvements Made\n\n1. **Robust Preprocessing Pipeline:** \n   - Used sklearn's ColumnTransformer for consistent preprocessing\n   - Applied OneHotEncoder instead of manual binary mapping\n   - Implemented proper missing value strategies\n\n2. **Feature Engineering:** \n   - Created meaningful derived features like Social Engagement Ratio and Activity Per Friend\n   - Developed an Introversion Score combining multiple behavioral indicators\n\n3. **Proper Model Selection:**\n   - Used RandomForestClassifier instead of RandomForestRegressor for classification\n   - Implemented comprehensive hyperparameter tuning\n   - Applied cross-validation for robust performance estimation\n\n4. **Comprehensive Evaluation:**\n   - Used appropriate classification metrics (accuracy, precision, recall, F1-score)\n   - Generated confusion matrix and ROC curve visualizations\n   - Analyzed feature importance to understand model decisions\n\n### Future Work Suggestions\n\n1. **Advanced Feature Engineering:**\n   - Explore polynomial features and interactions between existing features\n   - Investigate domain-specific features based on psychology research\n   - Consider temporal patterns if longitudinal data becomes available\n\n2. **Model Enhancements:**\n   - Experiment with ensemble methods (XGBoost, LightGBM)\n   - Try neural networks for potentially complex non-linear relationships\n   - Implement feature selection techniques to reduce overfitting\n\n3. **Data Collection:**\n   - Gather additional behavioral data (sleep patterns, communication styles)\n   - Collect validation data from psychological assessments\n   - Expand dataset size for better generalization\n\n4. **Deployment Considerations:**\n   - Implement model monitoring for performance drift\n   - Create confidence intervals for predictions\n   - Develop interpretability tools for end-users\n\n### Final Notes\n\nThis analysis demonstrates the importance of following machine learning best practices: proper data preprocessing, thoughtful feature engineering, appropriate model selection, and comprehensive evaluation. The resulting model provides a solid foundation for personality prediction while maintaining interpretability and reproducibility.\n\nThe combination of domain knowledge (understanding personality psychology) with technical expertise (proper ML pipeline implementation) has resulted in a robust and well-documented solution that can serve as a template for similar classification problems.",
      "metadata": {}
    }
  ]
}
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67e7c419",
   "metadata": {
    "papermill": {
     "duration": 0.007171,
     "end_time": "2026-01-01T15:26:45.039123",
     "exception": false,
     "start_time": "2026-01-01T15:26:45.031952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Hello**\n",
    "\n",
    "**This is a beginner friendly intermediate level notebook ;)**\n",
    "\n",
    "**Please do comment and spread your knowledge. Enlighten me and others.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3695c51",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-01T15:26:45.053237Z",
     "iopub.status.busy": "2026-01-01T15:26:45.052927Z",
     "iopub.status.idle": "2026-01-01T15:26:47.079516Z",
     "shell.execute_reply": "2026-01-01T15:26:47.078143Z"
    },
    "papermill": {
     "duration": 2.035569,
     "end_time": "2026-01-01T15:26:47.081272",
     "exception": false,
     "start_time": "2026-01-01T15:26:45.045703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/playground-series-s5e7/sample_submission.csv\n",
      "/kaggle/input/playground-series-s5e7/train.csv\n",
      "/kaggle/input/playground-series-s5e7/test.csv\n",
      "/kaggle/input/extrovert-vs-introvert-behavior-data/personality_datasert.csv\n",
      "/kaggle/input/extrovert-vs-introvert-behavior-data/personality_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00b6bc9",
   "metadata": {
    "papermill": {
     "duration": 0.005942,
     "end_time": "2026-01-01T15:26:47.093805",
     "exception": false,
     "start_time": "2026-01-01T15:26:47.087863",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ***So here we start.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "147e88ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T15:26:47.110713Z",
     "iopub.status.busy": "2026-01-01T15:26:47.110242Z",
     "iopub.status.idle": "2026-01-01T15:26:49.909036Z",
     "shell.execute_reply": "2026-01-01T15:26:49.908038Z"
    },
    "papermill": {
     "duration": 2.809069,
     "end_time": "2026-01-01T15:26:49.911042",
     "exception": false,
     "start_time": "2026-01-01T15:26:47.101973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#importing useful libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f0551c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T15:26:49.926254Z",
     "iopub.status.busy": "2026-01-01T15:26:49.925784Z",
     "iopub.status.idle": "2026-01-01T15:26:49.989430Z",
     "shell.execute_reply": "2026-01-01T15:26:49.988560Z"
    },
    "papermill": {
     "duration": 0.073209,
     "end_time": "2026-01-01T15:26:49.991232",
     "exception": false,
     "start_time": "2026-01-01T15:26:49.918023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X_full = X_full.dropna()'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading Datasets\n",
    "\n",
    "X_full = pd.read_csv('../input/playground-series-s5e7/train.csv')\n",
    "X_test = pd.read_csv('../input/playground-series-s5e7/test.csv')\n",
    "X_test_og = X_test\n",
    "\"\"\"X_full = X_full.dropna()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e44c453f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T15:26:50.005649Z",
     "iopub.status.busy": "2026-01-01T15:26:50.005300Z",
     "iopub.status.idle": "2026-01-01T15:26:50.019282Z",
     "shell.execute_reply": "2026-01-01T15:26:50.018064Z"
    },
    "papermill": {
     "duration": 0.0232,
     "end_time": "2026-01-01T15:26:50.021086",
     "exception": false,
     "start_time": "2026-01-01T15:26:49.997886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                             0\n",
       "Time_spent_Alone             425\n",
       "Stage_fear                   598\n",
       "Social_event_attendance      397\n",
       "Going_outside                466\n",
       "Drained_after_socializing    432\n",
       "Friends_circle_size          350\n",
       "Post_frequency               408\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97797451",
   "metadata": {
    "papermill": {
     "duration": 0.005883,
     "end_time": "2026-01-01T15:26:50.033523",
     "exception": false,
     "start_time": "2026-01-01T15:26:50.027640",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Some columns have categorical data and for now I don't have much expertize in dealing with categorical data, hence it is better to turn them into numerical ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc0e7d60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T15:26:50.049090Z",
     "iopub.status.busy": "2026-01-01T15:26:50.047998Z",
     "iopub.status.idle": "2026-01-01T15:26:50.108297Z",
     "shell.execute_reply": "2026-01-01T15:26:50.107017Z"
    },
    "papermill": {
     "duration": 0.070371,
     "end_time": "2026-01-01T15:26:50.110420",
     "exception": false,
     "start_time": "2026-01-01T15:26:50.040049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#updating categorical data into binary terms\n",
    "\n",
    "def turn_to_numeric(Dataframe, column, string):\n",
    "    new = Dataframe[column] == string\n",
    "    new += 0\n",
    "    Dataframe[column] = new\n",
    "\n",
    "# 1 means Yes, 0 implies No\n",
    "\"\"\"turn_to_numeric(X_full, 'Stage_fear', 'No')\n",
    "turn_to_numeric(X_full, 'Drained_after_socializing', \"Yes\")\n",
    "turn_to_numeric(X_test, 'Stage_fear', 'No')\n",
    "turn_to_numeric(X_test, 'Drained_after_socializing', 'Yes')\n",
    "\"\"\"\n",
    "\n",
    "X_full = X_full.sort_values(by = 'Stage_fear')\n",
    "X_test = X_test.sort_values(by = 'Stage_fear')\n",
    "X_full['Stage_fear'] = pd.Categorical(X_full['Stage_fear']).codes\n",
    "X_test['Stage_fear'] = pd.Categorical(X_test['Stage_fear']).codes\n",
    "\n",
    "X_full = X_full.sort_values(by = 'Drained_after_socializing')\n",
    "X_test = X_test.sort_values(by = 'Drained_after_socializing')\n",
    "X_full['Drained_after_socializing'] = pd.Categorical(X_full['Drained_after_socializing']).codes\n",
    "X_test['Drained_after_socializing'] = pd.Categorical(X_test['Drained_after_socializing']).codes\n",
    "\n",
    "X_full = X_full.sort_values(by = 'id')\n",
    "X_test = X_test.sort_values(by = 'id')\n",
    "\n",
    "# 1 means Exrovert and 0 refers to Introvert\n",
    "turn_to_numeric(X_full, 'Personality', 'Extrovert')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269360b0",
   "metadata": {
    "papermill": {
     "duration": 0.006325,
     "end_time": "2026-01-01T15:26:50.123227",
     "exception": false,
     "start_time": "2026-01-01T15:26:50.116902",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Creating a new feature using drained after socializing and friends circle size\n",
    "\n",
    "* An introvert can have a good friends circle size, but will get drained after socializing\n",
    "\n",
    "There must be a relation between time spent alone and going out.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cfbc4d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T15:26:50.137479Z",
     "iopub.status.busy": "2026-01-01T15:26:50.136452Z",
     "iopub.status.idle": "2026-01-01T15:26:50.147402Z",
     "shell.execute_reply": "2026-01-01T15:26:50.146301Z"
    },
    "papermill": {
     "duration": 0.019642,
     "end_time": "2026-01-01T15:26:50.149104",
     "exception": false,
     "start_time": "2026-01-01T15:26:50.129462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_full['new1'] = X_full.Drained_after_socializing * X_full.Friends_circle_size\n",
    "X_test['new1'] = X_test.Drained_after_socializing * X_test.Friends_circle_size\n",
    "\n",
    "X_full['new2'] = X_full.Time_spent_Alone + X_full.Going_outside\n",
    "X_test['new2'] = X_test.Time_spent_Alone + X_test.Going_outside\n",
    "\n",
    "X_full['new3'] = X_full.Drained_after_socializing * X_full.Stage_fear\n",
    "X_test['new3'] = X_test.Drained_after_socializing * X_test.Stage_fear\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f8cd399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T15:26:50.163196Z",
     "iopub.status.busy": "2026-01-01T15:26:50.162518Z",
     "iopub.status.idle": "2026-01-01T15:26:50.168457Z",
     "shell.execute_reply": "2026-01-01T15:26:50.167286Z"
    },
    "papermill": {
     "duration": 0.014595,
     "end_time": "2026-01-01T15:26:50.170100",
     "exception": false,
     "start_time": "2026-01-01T15:26:50.155505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"X_full = X_full.drop(['Post_frequency'], axis = 1)\\nX_test = X_test.drop(['Post_frequency'], axis = 1)\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"X_full = X_full.drop(['Post_frequency'], axis = 1)\n",
    "X_test = X_test.drop(['Post_frequency'], axis = 1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "287cb96c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T15:26:50.184604Z",
     "iopub.status.busy": "2026-01-01T15:26:50.184277Z",
     "iopub.status.idle": "2026-01-01T15:26:50.197455Z",
     "shell.execute_reply": "2026-01-01T15:26:50.196439Z"
    },
    "papermill": {
     "duration": 0.022605,
     "end_time": "2026-01-01T15:26:50.199163",
     "exception": false,
     "start_time": "2026-01-01T15:26:50.176558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                           171560026.0\n",
       "Time_spent_Alone                649390.0\n",
       "Stage_fear                        2129.0\n",
       "Social_event_attendance         681318.0\n",
       "Going_outside                   801988.0\n",
       "Drained_after_socializing         2913.0\n",
       "Friends_circle_size             666703.0\n",
       "Post_frequency                  717991.0\n",
       "Personality                      13699.0\n",
       "new1                            531121.0\n",
       "new2                           1416049.0\n",
       "new3                              1979.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full = X_full.fillna(500)\n",
    "X_test = X_test.fillna(500)\n",
    "X_full.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25319d2b",
   "metadata": {
    "papermill": {
     "duration": 0.005987,
     "end_time": "2026-01-01T15:26:50.211698",
     "exception": false,
     "start_time": "2026-01-01T15:26:50.205711",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Finally splitting the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c191c00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T15:26:50.225839Z",
     "iopub.status.busy": "2026-01-01T15:26:50.225492Z",
     "iopub.status.idle": "2026-01-01T15:26:50.238599Z",
     "shell.execute_reply": "2026-01-01T15:26:50.237470Z"
    },
    "papermill": {
     "duration": 0.022043,
     "end_time": "2026-01-01T15:26:50.240375",
     "exception": false,
     "start_time": "2026-01-01T15:26:50.218332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Removing not needed columns before splitting the data\n",
    "\n",
    "y = X_full[\"Personality\"]\n",
    "X = X_full.drop([\"id\", \"Personality\"], axis = 1)\n",
    "\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                      random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bd2326",
   "metadata": {
    "papermill": {
     "duration": 0.006222,
     "end_time": "2026-01-01T15:26:50.253596",
     "exception": false,
     "start_time": "2026-01-01T15:26:50.247374",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651d6366",
   "metadata": {
    "papermill": {
     "duration": 0.006319,
     "end_time": "2026-01-01T15:26:50.266393",
     "exception": false,
     "start_time": "2026-01-01T15:26:50.260074",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Imputation for the NAN values.\n",
    "Since the test data also contains NAN values, it is not a good idea to drop that data.\n",
    "\n",
    "I will also check in some other version for putting 0 or something like that for NAN values. In that case, I will need to increase the weights of other values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab18c1e",
   "metadata": {
    "papermill": {
     "duration": 0.006039,
     "end_time": "2026-01-01T15:26:50.279063",
     "exception": false,
     "start_time": "2026-01-01T15:26:50.273024",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "While there are ways by which we may create models without the need for imputation.\n",
    "The link to the discussion where it is mentioned is : [How to go without imputing the values](https://www.kaggle.com/competitions/playground-series-s5e7/discussion/593875) suggestions provided by [Sylvester d'Almeida](https://www.kaggle.com/sylvesterdalmeida).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58baa077",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T15:26:50.293723Z",
     "iopub.status.busy": "2026-01-01T15:26:50.293188Z",
     "iopub.status.idle": "2026-01-01T15:26:50.315583Z",
     "shell.execute_reply": "2026-01-01T15:26:50.314458Z"
    },
    "papermill": {
     "duration": 0.031887,
     "end_time": "2026-01-01T15:26:50.317353",
     "exception": false,
     "start_time": "2026-01-01T15:26:50.285466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_imputer = SimpleImputer()\n",
    "\n",
    "\n",
    "cols_with_missing = [col for col in X_train.columns\n",
    "                     if X_train[col].isnull().any()]\n",
    "\n",
    "for col in cols_with_missing:\n",
    "    X_train[col + '_was_missing'] = X_train[col].isnull()\n",
    "    X_valid[col + '_was_missing'] = X_valid[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585f933d",
   "metadata": {
    "papermill": {
     "duration": 0.007153,
     "end_time": "2026-01-01T15:26:50.331223",
     "exception": false,
     "start_time": "2026-01-01T15:26:50.324070",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80e631cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T15:26:50.345928Z",
     "iopub.status.busy": "2026-01-01T15:26:50.345572Z",
     "iopub.status.idle": "2026-01-01T15:26:54.403715Z",
     "shell.execute_reply": "2026-01-01T15:26:54.402575Z"
    },
    "papermill": {
     "duration": 4.06732,
     "end_time": "2026-01-01T15:26:54.405339",
     "exception": false,
     "start_time": "2026-01-01T15:26:50.338019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=350, random_state=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=350, random_state=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=350, random_state=3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### model creation\n",
    "model = RandomForestClassifier(n_estimators = 350, random_state = 3)\n",
    "model.fit(imputed_X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f4cffa",
   "metadata": {
    "papermill": {
     "duration": 0.006483,
     "end_time": "2026-01-01T15:26:54.418787",
     "exception": false,
     "start_time": "2026-01-01T15:26:54.412304",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Valiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80a2db",
   "metadata": {
    "papermill": {
     "duration": 0.006463,
     "end_time": "2026-01-01T15:26:54.432058",
     "exception": false,
     "start_time": "2026-01-01T15:26:54.425595",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Model is created, let's now test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12d19926",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T15:26:54.446581Z",
     "iopub.status.busy": "2026-01-01T15:26:54.446273Z",
     "iopub.status.idle": "2026-01-01T15:26:54.681956Z",
     "shell.execute_reply": "2026-01-01T15:26:54.681031Z"
    },
    "papermill": {
     "duration": 0.244811,
     "end_time": "2026-01-01T15:26:54.683622",
     "exception": false,
     "start_time": "2026-01-01T15:26:54.438811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = model.predict(imputed_X_valid)\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47fae017",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T15:26:54.698527Z",
     "iopub.status.busy": "2026-01-01T15:26:54.698239Z",
     "iopub.status.idle": "2026-01-01T15:26:54.704377Z",
     "shell.execute_reply": "2026-01-01T15:26:54.703479Z"
    },
    "papermill": {
     "duration": 0.015448,
     "end_time": "2026-01-01T15:26:54.705879",
     "exception": false,
     "start_time": "2026-01-01T15:26:54.690431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7425101214574898"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d846d9cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T15:26:54.722165Z",
     "iopub.status.busy": "2026-01-01T15:26:54.721858Z",
     "iopub.status.idle": "2026-01-01T15:26:54.736309Z",
     "shell.execute_reply": "2026-01-01T15:26:54.735341Z"
    },
    "papermill": {
     "duration": 0.025081,
     "end_time": "2026-01-01T15:26:54.738169",
     "exception": false,
     "start_time": "2026-01-01T15:26:54.713088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In case of regression models such thing would have been needed.\n",
    "# Since it is a classification problem.\n",
    "for i in range(len(test_pred)):\n",
    "    test_pred[i] = 1 if test_pred[i]>= 0.7 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27a2387",
   "metadata": {
    "papermill": {
     "duration": 0.006581,
     "end_time": "2026-01-01T15:26:54.752480",
     "exception": false,
     "start_time": "2026-01-01T15:26:54.745899",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Earlier when I used regression model**\n",
    "\n",
    "(Our model made really nice predictions, but they were all values ranging from 0 to 1. While we just wanted only two values that are 0 and 1. So we converted those having value 0.5 and above to 1.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82166cae",
   "metadata": {
    "papermill": {
     "duration": 0.006443,
     "end_time": "2026-01-01T15:26:54.765779",
     "exception": false,
     "start_time": "2026-01-01T15:26:54.759336",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Nice!! But we will require some thing to measure if our model really worked well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6656991",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T15:26:54.848509Z",
     "iopub.status.busy": "2026-01-01T15:26:54.848201Z",
     "iopub.status.idle": "2026-01-01T15:26:54.858005Z",
     "shell.execute_reply": "2026-01-01T15:26:54.857157Z"
    },
    "papermill": {
     "duration": 0.019423,
     "end_time": "2026-01-01T15:26:54.859742",
     "exception": false,
     "start_time": "2026-01-01T15:26:54.840319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9665317139001349"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_valid, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b2a9d9",
   "metadata": {
    "papermill": {
     "duration": 0.006891,
     "end_time": "2026-01-01T15:26:54.873880",
     "exception": false,
     "start_time": "2026-01-01T15:26:54.866989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The accuracy check can also be done manually as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8b21471",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T15:26:54.889905Z",
     "iopub.status.busy": "2026-01-01T15:26:54.889539Z",
     "iopub.status.idle": "2026-01-01T15:26:54.897106Z",
     "shell.execute_reply": "2026-01-01T15:26:54.896013Z"
    },
    "papermill": {
     "duration": 0.017861,
     "end_time": "2026-01-01T15:26:54.898750",
     "exception": false,
     "start_time": "2026-01-01T15:26:54.880889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.6531713900135"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = sum((y_valid - test_pred) !=0)\n",
    "100 - (n / len(y_valid) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a183a",
   "metadata": {
    "papermill": {
     "duration": 0.006924,
     "end_time": "2026-01-01T15:26:54.912845",
     "exception": false,
     "start_time": "2026-01-01T15:26:54.905921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training on Whole Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f670aa",
   "metadata": {
    "papermill": {
     "duration": 0.006908,
     "end_time": "2026-01-01T15:26:54.927537",
     "exception": false,
     "start_time": "2026-01-01T15:26:54.920629",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now as we know how our model need to be, why not train it on whole train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "552ab24e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T15:26:54.943104Z",
     "iopub.status.busy": "2026-01-01T15:26:54.942773Z",
     "iopub.status.idle": "2026-01-01T15:27:00.071276Z",
     "shell.execute_reply": "2026-01-01T15:27:00.070131Z"
    },
    "papermill": {
     "duration": 5.138737,
     "end_time": "2026-01-01T15:27:00.073402",
     "exception": false,
     "start_time": "2026-01-01T15:26:54.934665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=350, random_state=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=350, random_state=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=350, random_state=3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_X_full = pd.DataFrame(my_imputer.fit_transform(X_full))\n",
    "imputed_X_full.columns = X_full.columns\n",
    "imputed_X_full = imputed_X_full.drop(['id', 'Personality'], axis = 1)\n",
    "\n",
    "model.fit(imputed_X_full, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d5488a",
   "metadata": {
    "papermill": {
     "duration": 0.008035,
     "end_time": "2026-01-01T15:27:00.091361",
     "exception": false,
     "start_time": "2026-01-01T15:27:00.083326",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Great!!! Now let's make our model work on real test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f66dd06f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T15:27:00.109118Z",
     "iopub.status.busy": "2026-01-01T15:27:00.108800Z",
     "iopub.status.idle": "2026-01-01T15:27:00.135846Z",
     "shell.execute_reply": "2026-01-01T15:27:00.134784Z"
    },
    "papermill": {
     "duration": 0.037848,
     "end_time": "2026-01-01T15:27:00.137446",
     "exception": false,
     "start_time": "2026-01-01T15:27:00.099598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Time_spent_Alone</th>\n",
       "      <th>Stage_fear</th>\n",
       "      <th>Social_event_attendance</th>\n",
       "      <th>Going_outside</th>\n",
       "      <th>Drained_after_socializing</th>\n",
       "      <th>Friends_circle_size</th>\n",
       "      <th>Post_frequency</th>\n",
       "      <th>new1</th>\n",
       "      <th>new2</th>\n",
       "      <th>new3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18524</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18525</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18526</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18527</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18528</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  Time_spent_Alone  Stage_fear  Social_event_attendance  \\\n",
       "0  18524               3.0           0                      7.0   \n",
       "1  18525             500.0           1                      0.0   \n",
       "2  18526               3.0           0                      5.0   \n",
       "3  18527               3.0           0                      4.0   \n",
       "4  18528               9.0           1                      1.0   \n",
       "\n",
       "   Going_outside  Drained_after_socializing  Friends_circle_size  \\\n",
       "0            4.0                          0                  6.0   \n",
       "1            0.0                          1                  5.0   \n",
       "2            6.0                          0                 15.0   \n",
       "3            4.0                          0                  5.0   \n",
       "4            2.0                          1                  1.0   \n",
       "\n",
       "   Post_frequency  new1   new2  new3  \n",
       "0           500.0   0.0    7.0     0  \n",
       "1             1.0   5.0  500.0     1  \n",
       "2             9.0   0.0    9.0     0  \n",
       "3             6.0   0.0    7.0     0  \n",
       "4             1.0   1.0   11.0     1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking a glance at the test data\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2362270",
   "metadata": {
    "papermill": {
     "duration": 0.007591,
     "end_time": "2026-01-01T15:27:00.153191",
     "exception": false,
     "start_time": "2026-01-01T15:27:00.145600",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The test data has some missing values, so let's impute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fb339fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T15:27:00.169416Z",
     "iopub.status.busy": "2026-01-01T15:27:00.169082Z",
     "iopub.status.idle": "2026-01-01T15:27:00.587368Z",
     "shell.execute_reply": "2026-01-01T15:27:00.586496Z"
    },
    "papermill": {
     "duration": 0.428499,
     "end_time": "2026-01-01T15:27:00.589205",
     "exception": false,
     "start_time": "2026-01-01T15:27:00.160706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_X_test = pd.DataFrame(my_imputer.fit_transform(X_test))\n",
    "imputed_X_test.columns = X_test.columns\n",
    "imputed_X_test = imputed_X_test.drop(['id'], axis = 1)\n",
    "final_prediction = model.predict(imputed_X_test)\n",
    "\n",
    "final_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b51b54",
   "metadata": {
    "papermill": {
     "duration": 0.007215,
     "end_time": "2026-01-01T15:27:00.604058",
     "exception": false,
     "start_time": "2026-01-01T15:27:00.596843",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Final Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ab390",
   "metadata": {
    "papermill": {
     "duration": 0.007552,
     "end_time": "2026-01-01T15:27:00.619052",
     "exception": false,
     "start_time": "2026-01-01T15:27:00.611500",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Final Predictions are made, now let's convert it into the asked format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "797a0e98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T15:27:00.635607Z",
     "iopub.status.busy": "2026-01-01T15:27:00.635249Z",
     "iopub.status.idle": "2026-01-01T15:27:00.668944Z",
     "shell.execute_reply": "2026-01-01T15:27:00.667444Z"
    },
    "papermill": {
     "duration": 0.044157,
     "end_time": "2026-01-01T15:27:00.670815",
     "exception": false,
     "start_time": "2026-01-01T15:27:00.626658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "list = []\n",
    "for i in range(len(final_prediction)):\n",
    "    if final_prediction[i]>=0.7:\n",
    "        list.append(\"Extrovert\")\n",
    "    else :\n",
    "        list.append(\"Introvert\")\n",
    "\n",
    "output = pd.DataFrame({'id': X_test_og.id,\n",
    "                       'Personality': list})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e07a0ef",
   "metadata": {
    "papermill": {
     "duration": 0.007097,
     "end_time": "2026-01-01T15:27:00.685657",
     "exception": false,
     "start_time": "2026-01-01T15:27:00.678560",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Awesome!! That's it, just one more line of code. (To see how our output looks like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f19740f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T15:27:00.702787Z",
     "iopub.status.busy": "2026-01-01T15:27:00.702410Z",
     "iopub.status.idle": "2026-01-01T15:27:00.715618Z",
     "shell.execute_reply": "2026-01-01T15:27:00.714753Z"
    },
    "papermill": {
     "duration": 0.023158,
     "end_time": "2026-01-01T15:27:00.717240",
     "exception": false,
     "start_time": "2026-01-01T15:27:00.694082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personality</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Extrovert</th>\n",
       "      <td>4615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Introvert</th>\n",
       "      <td>1560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id\n",
       "Personality      \n",
       "Extrovert    4615\n",
       "Introvert    1560"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)\n",
    "output.groupby(\"Personality\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45ae124",
   "metadata": {
    "papermill": {
     "duration": 0.007244,
     "end_time": "2026-01-01T15:27:00.732322",
     "exception": false,
     "start_time": "2026-01-01T15:27:00.725078",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Thank you for coming down till here.\n",
    "\n",
    "If you reached here, do consider giving an upvote."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12738969,
     "sourceId": 91718,
     "sourceType": "competition"
    },
    {
     "datasetId": 7474089,
     "sourceId": 12156348,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 22.404025,
   "end_time": "2026-01-01T15:27:01.459834",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-01T15:26:39.055809",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
